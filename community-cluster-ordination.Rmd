---
title: "Lab 2 - Community"
author: "Peter Menzies"
date: "1/30/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

librarian::shelf(
  dplyr, DT, ggplot2, tibble)
```

# Clustering

```{r}
# set seed for reproducible results
set.seed(42)

# load the dataset
data("iris")

# look at documentation in RStudio
if (interactive())
  help(iris)

# show data table
datatable(iris)
```

```{r}
# plot petal length vs width, species naive
ggplot(
  iris, aes(Petal.Length, Petal.Width)) +
  geom_point()
```

```{r}
# plot petal length vs width, color by species
legend_pos <- theme(
    legend.position = c(0.95, 0.05),
    legend.justification = c("right", "bottom"),
    legend.box.just = "right")
ggplot(
  iris, aes(Petal.Length, Petal.Width, color = Species)) +
  geom_point() +
  legend_pos
```

## Cluster `iris` using `kmeans`

```{r}
# cluster using kmeans - defaults to 10 max iterations allowed
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# show cluster result
iris_k
```

```{r}
# compare clusters with species (which were not used to cluster)
table(iris_k$cluster, iris$Species)
```

Question: How many observations could be considered “misclassified” if expecting petal length and width to differentiate between species?

Answer: **6**

```{r}
# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos
```

Question: Comparing the observed species plot with 3 species with the kmeans() cluster plot with 3 clusters, where does this “unsupervised” kmeans() technique (that does not use species to “fit” the model) produce similar versus different results? One or two sentences would suffice. Feel free to mention ranges of values along the axes.

**Overall, the `kmeans()` clustering performs very accurately. From $x$ = ~1 to ~4.5 and $x$ > ~5.5, the clusters are identical. From $x$ = ~4.5 to ~5.5, `kmeans()` classified things a bit differently---this is unsurprising as the true *versicolor* and *virginica* clusters overlap slightly within that interval.**

```{r}
librarian::shelf(ggvoronoi, scales)
```

## Plot Voronoi diagram of clustered `iris`

```{r}
# define bounding box for geom_voronoi()
box <- tribble(
  ~Petal.Length, ~Petal.Width, ~group,
  1, 0.1, 1,
  1, 2.5, 1,
  7, 2.5, 1,
  7, 0.1, 1,
  1, 0.1, 1) %>% 
  data.frame()

# cluster using kmeans
k <- 3  # number of clusters
iris_k <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

Task: Show the Voronoi diagram for fewer (k=2) and more (k=8) clusters to see how assignment to cluster centroids work.


```{r}
# cluster using kmeans
k <- 2  # number of clusters
iris_k_2 <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k_2$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k_2$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

```{r}
# cluster using kmeans
k <- 8  # number of clusters
iris_k_8 <- kmeans(
  iris %>% 
    select(Petal.Length, Petal.Width), 
  centers = k)

# extract cluster assignment per observation
Cluster = factor(iris_k_8$cluster)

# extract cluster centers
ctrs <- as.data.frame(iris_k_8$centers) %>% 
  mutate(
    Cluster = factor(1:k))

# plot points with voronoi diagram showing nearest centroid
ggplot(iris, aes(Petal.Length, Petal.Width, color = Cluster)) +
  geom_point() + 
  legend_pos +
  geom_voronoi(
    data = ctrs, aes(fill=Cluster), color = NA, alpha=0.5, outline = box) + 
  geom_point(
    data = ctrs, pch=23, cex=2, fill="black")
```

## Hierarchical Clustering

```{r}
librarian::shelf(
  cluster, vegan)
```

```{r}
# load dune dataset from package vegan
data("dune")

# show documentation on dataset if interactive
if (interactive())
  help(dune)
```

Question: In your own words, how does Bray Curtis differ from Euclidean distance? See sites_euclidean versus sites_bray from lab code, slides from Lecture 05. Clustering and reading Chapter 8 of Kindt and Coe (2005).

**Euclidean distance treats species as axes and each site is plotted according to the abundance of species corresponding with each axis---the distance is the deemed to be the actual 'length' of the distance between the points plotted on some n-dimensional plane/hyperplane. Bray-Curtis distances are calculated from differences in abundance of each species, and unlike Euclidean distance, are constrained between 0 and 1---0 being most similar, and 1 lacking any species overlap.**

Question: What are the rows and columns composed of in the dune data frame?

**The columns represent 30 different veg species, and the rows represent 20 different sites.**

## Calculate Ecological Distances on `sites`

```{r}
sites <- tribble(
  ~site, ~sp1, ~sp2, ~sp3,
    "A",    1,    1,    0,
    "B",    5,    5,    0,
    "C",    0,    0,    1) %>% 
  column_to_rownames("site")
sites
```

```{r}
sites_manhattan <- vegdist(sites, method="manhattan")
sites_manhattan
```

```{r}
sites_euclidean <- vegdist(sites, method="euclidean")
sites_euclidean
```

```{r}
sites_bray <- vegdist(sites, method="bray")
sites_bray
```

## Agglomerative hierarchical clustering on `dune`

```{r, results='hide'}
# Dissimilarity matrix
d <- vegdist(dune, method="bray")
dim(d)
```

```{r}
as.matrix(d)[1:5, 1:5]
```

```{r}
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Dendrogram plot of hc1
plot(hc1, cex = 0.6, hang = -1)
```

Question: Which function comes first, vegdist() or hclust(), and why?

**`vegdist()` comes first becuase `vegdist()` calculates the ecological distances between each site. Distances are the input required by `hclust()`, which it uses to build the hierarchical structure.**

```{r}
# Compute agglomerative clustering with agnes
hc2 <- agnes(dune, method = "complete")

# Agglomerative coefficient
hc2$ac
```

Question: In your own words how does hclust() differ from agnes()?

**`agnes()`, like `hclust()` performs agglomerative hierarchical clustering, but it is able to accept a dataframe as input because it has distance functionality built into it, *and* it provides an agglomerative coefficient which describes the amount of clustering structure.**

```{r}
# Dendrogram plot of hc2
plot(hc2, which.plot = 2)
```

```{r}
# methods to assess
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# function to compute coefficient
ac <- function(x) {
  agnes(dune, method = x)$ac
}

# get agglomerative coefficient for each linkage method
purrr::map_dbl(m, ac)
```

Question: Of the 4 methods, which is the “best” model in terms of Agglomerative Coefficient?

**Based on the ACs returned, ward appears to be the best model.**

```{r}
# Compute ward linkage clustering with agnes
hc3 <- agnes(dune, method = "ward")

# Agglomerative coefficient
hc3$ac
```

## Divisive hierarchical clustering on `dune`

```{r}
# compute divisive hierarchical clustering
hc4 <- diana(dune)

# Divise coefficient; amount of clustering structure found
hc4$dc
```

Question: In your own words how does agnes() differ from diana()?

**`agnes()` performs agglomerative hierarchical clustering (bottom to top), while `diana()` performs divisive hierarchical clustering (top to bottom).**

```{r}
librarian::shelf(factoextra)
```

```{r}
# Plot cluster results
p1 <- fviz_nbclust(dune, FUN = hcut, method = "wss",  k.max = 10) +
  ggtitle("(A) Elbow method")

p2 <- fviz_nbclust(dune, FUN = hcut, method = "silhouette", k.max = 10) +
  ggtitle("(B) Silhouette method")

p3 <- fviz_nbclust(dune, FUN = hcut, method = "gap_stat", k.max = 10) +
  ggtitle("(C) Gap statistic")

# Display plots side by side
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

Question: How do the optimal number of clusters compare between methods for those with a dashed line?

**The silhouette method suggests 4 clusters as optimal whereas the gap statistic method suggests 3.**

Question: In dendrogram plots, which is the biggest determinant of relatedness between observations: the distance between observations along the labeled axes or the height of their shared connection?

**The height of the shared connection between observations is the primary indicator of relatedness.**

```{r}
# Construct dendorgram for the Ames housing example
hc5 <- hclust(d, method = "ward.D2" )
dend_plot <- fviz_dend(hc5)

dend_data <- attr(dend_plot, "dendrogram")
dend_cuts <- cut(dend_data, h = 8)
fviz_dend(dend_cuts$lower[[2]])
```

```{r}
# Ward's method
hc5 <- hclust(d, method = "ward.D2" )

# Cut tree into 4 groups
k = 4
sub_grp <- cutree(hc5, k = k)

# Number of members in each cluster
table(sub_grp)
```

```{r}
# Plot full dendogram
fviz_dend(
  hc5,
  k = k,
  horiz = TRUE,
  rect = TRUE,
  rect_fill = TRUE,
  rect_border = "jco",
  k_colors = "jco")
```


# Ordination

## Principal Component Analysis

```{r}
# load R packages
librarian::shelf(
  dplyr, ggplot2, h2o)
```

```{r}
# set seed for reproducible results
set.seed(42)

# get data
url <- "https://koalaverse.github.io/homlr/data/my_basket.csv"
my_basket <- readr::read_csv(url)
dim(my_basket)
```

```{r}
my_basket
```

## Performing PCA in R

```{r}
h2o.no_progress()  # turn off progress bars for brevity
h2o.init(max_mem_size = "5g")  # connect to H2O instance
```

```{r}
# convert data to h2o object
my_basket.h2o <- as.h2o(my_basket)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000)
my_pca
```

Question: Why is the pca_method of “GramSVD” chosen over “GLRM”?

**GramSVD is used here because the data are numeric---GLRM is better when the dataset contains more categorical variables.**

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point()
```

Question: How many initial principal components are chosen with respect to dimensions of the input data?

**42**

```{r}
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, pc2, label = feature)) +
  geom_text()
```

Question: What category of grocery items contribute most to PC1?

**Alcoholic beverages**

Question: What category of grocery items contribute the least to PC1 but positively towards PC2?

**Vegetables**

### Eigenvalue criterion

```{r}
# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
```


```{r}
# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
```

```{r}
# Extract PVE and CVE
ve <- data.frame(
  PC  = my_pca@model$importance %>% seq_along(),
  PVE = my_pca@model$importance %>% .[2,] %>% unlist(),
  CVE = my_pca@model$importance %>% .[3,] %>% unlist())

# Plot PVE and CVE
ve %>%
  tidyr::gather(metric, variance_explained, -PC) %>%
  ggplot(aes(PC, variance_explained)) +
  geom_point() +
  facet_wrap(~ metric, ncol = 1, scales = "free")
```

Question: How many principal components would you include to explain 90% of the total variance?

**35 principal components which will explain ~89.83% of the variance.**

```{r}
# How many PCs required to explain at least 75% of total variability
min(which(ve$CVE >= 0.75))
```

```{r}
# Scree plot criterion
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)
```

Question: How many principal components to include up to the elbow of the PVE, i.e. the “elbow” before plateau of dimensions explaining the least variance?

**8 principal components. At that point the proportion of variance explained flattens considerably.**

Question: What are a couple of disadvantages to using PCA?

**Some disadvantages of using PCA include: PCA can be greatly impacted by outliers, and PCA doesn't perform well when nonlinear relationships exist as is often in the case with highly dimensional datasets.**

## Non-metric MultiDimensional Scaling (NMDS)

### Unconstrained ordination on species

```{r}
# load R packages
librarian::shelf(
  vegan, vegan3d)
```

```{r}
# vegetation and environment in lichen pastures from Vare et al (1995)
data("varespec") # species
data("varechem") # chemistry

varespec %>% tibble()
```

Question: What are the dimensions of the varespec data frame and what do rows versus columns represent?

**The `varespec` df has 24 rows and 44 columns---the columns are different veg species and the rows are estimated cover values in different lichen pasture sites.**

```{r}
vare.dis <- vegdist(varespec)
vare.mds0 <- monoMDS(vare.dis)
stressplot(vare.mds0)
```

Question: The “stress” in a stressplot represents the difference between the observed inpnut distance versus the fitted ordination distance. How much better is the non-metric (i.e., NMDS) fit versus a linear fit (as with PCA) in terms of \(R^2\)?

**Based on the $R^2$ values in the stressplot, the nonmetric fit explains 4.7% more of the variation than the linear fit.**

```{r}
ordiplot(vare.mds0, type = "t")
```

Question: What two sites are most dissimilar based on species composition for the first component MDS1? And two more most dissimilar sites for the second component MDS2?

**Sites 5 and 28 are most dissimilar for MDS1; sites 14 and 21 are most dissimilar for MDS2.**

```{r}
vare.mds <- metaMDS(varespec, trace = FALSE)
vare.mds
```


```{r}
plot(vare.mds, type = "t")
```

Question: What is the basic difference between metaMDS and monoMDS()?

**The `monoMDS()` function takes dissimilarities as input and uses a single random configuration as the starting solution and iterates from there. The `metaMDS()` function, on the other hand, can take the original dataframe as input and uses several random starts---this is a better approach for finding the global optimum and not just a local optimum which can happen with `monoMDS()`.**


```{r}
ef <- envfit(vare.mds, varechem, permu = 999)
ef
```


```{r}
plot(vare.mds, display = "sites")
plot(ef, p.max = 0.05)
```

Question: What two soil chemistry elements have the strongest negative relationship with NMDS1 that is based on species composition?

**Aluminum and iron**

```{r}
ef <- envfit(vare.mds ~ Al + Ca, data = varechem)
plot(vare.mds, display = "sites")
plot(ef)

tmp <- with(varechem, ordisurf(vare.mds, Al, add = TRUE))
ordisurf(vare.mds ~ Ca, data=varechem, add = TRUE, col = "green4")
```

Question: Describe in general terms (upper/lower/left/right/middle) where the highest and lowest values are found for Ca with respect to the ordination axes NMDS1 and NMDS2.

**The highest values for CA (with respect to the ordination axes) are found in the upper-middle(ish) and the lowest values are found in the lower-left.**

### Constrained Ordination on Species and Environment

```{r}
# ordinate on species constrained by three soil elements
vare.cca <- cca(varespec ~ Al + P + K, varechem)
vare.cca
```

Question: What is the difference between “constrained” versus “unconstrained” ordination within ecological context?

**Unconstrained ordination looks first for compositional variation and then relates that to environmental variation; whereas, constrained ordination seeks to specifically display the variation explained by the environmental variables of interest.**

```{r}
# plot ordination
plot(vare.cca)
```

Question: What sites are most differentiated by CCA1, i.e. furthest apart along its axis, based on species composition AND the environmnent? What is the strongest environmental vector for CCA1, i.e. longest environmental vector in the direction of the CCA1 axes?

**Sites 4 and 28 are the most differentiated by `CCA1`. Aluminum is the strongest environmental vector for `CCA1`**

```{r}
# plot 3 dimensions
ordiplot3d(vare.cca, type = "h")
```


```{r}
if (interactive()){
  ordirgl(vare.cca)
}
```



